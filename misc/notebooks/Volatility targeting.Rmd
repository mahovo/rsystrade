---
title: "Notes on volatility targeting in rsystrade"
author: "mhv"
date: "2023-02-16"
output: 
  rmarkdown::github_document:
    toc: true
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
library(tidyverse)
library(ggplot2)
library(fGarch)
```

(See Obsidian note "Volatility Targeting")  

The more interesting stuff is way down below...  

## Volatility targeting
Example: We generate a signal with approx mean 2 and approx volatility 4.   
We want to rescale the volatility to 1 and we want a mean of 10  

Create vector of returns  
```{r}
set.seed(3746)
ret <- rnorm(1000, 2, 4)
```

Get mean - should be close to 2  
```{r}
mean(ret)
```


Get volatility - should be close to 4  
```{r}
sd(ret)
```

Standardise returns  
```{r}
std_ret <- ret / sd(ret)
```

Get mean of standardised returns  
```{r}
mean(std_ret)
```
- The mean was of course reduced when the volatility was reduced  

Get volatility of standardised returns -  should be 1  
```{r}
sd(std_ret)
```

```{r, echo=FALSE}
ret1 <- sort(c(ret, mean(ret)))
std_ret1 <- sort(c(std_ret, mean(std_ret)))

plot(ret1, pch = 16, cex = 0.3, col = "black")
points(std_ret1, pch = 16, cex = 0.3, col = "red")
abline(v=which(ret1 == mean(ret1)), lty = "solid", lwd = 5, col="black")
abline(v=which(std_ret1 == mean(std_ret1)), lty = "solid", lwd = 2, col="red")
```


So what if we want the same mean but standardised volatility?  
First we shift the data to get a mean of 0:  
```{r}
demeaned_ret <- ret - mean(ret)
```

Check the mean of the "demeaned" returns - should be 0:  
```{r}
mean(demeaned_ret)
```

Check the volatility of the "demeaned" returns -  should be unchanged (i.e. 4.073677)  
```{r}
sd(demeaned_ret)
```

Now standardise the demeaned returns  
```{r}
std_demeaned_ret <- demeaned_ret / sd(demeaned_ret)
```

Check mean of standardised demeaned returns - should be 0  
```{r}
mean(std_demeaned_ret)
```

Check volatility of std_demeaned_ret - should be 1  
```{r}
sd(std_demeaned_ret)
```

Now shift the mean of the standardised demeaned returns to our target volatility of 10.  
```{r}
normalized_ret <- std_demeaned_ret + 10
```

Check mean of normalized returns - should be 10  
```{r}
mean(normalized_ret)
```

Check volatility of normalized returns - should be 1  
```{r}
sd(normalized_ret)
```

```{r, echo=FALSE}
ret1 <- sort(c(ret, mean(ret)))
demeaned_ret1 <- sort(c(demeaned_ret, mean(demeaned_ret)))
std_demeaned_ret1 <- sort(c(std_demeaned_ret, mean(std_demeaned_ret)))
normalized_ret1 <- sort(c(normalized_ret, mean(normalized_ret)))


plot(ret1, pch = 16, cex = 0.3, col = "black")
points(demeaned_ret1, pch = 16, cex = 0.3, col = "green")
points(std_demeaned_ret1, pch = 16, cex = 0.3, col = "blue")
points(normalized_ret1, pch = 16, cex = 0.3, col = "orange")
abline(v=which(ret1 == mean(ret1)), lty = "solid", lwd = 5, col="black")
abline(v=which(demeaned_ret1 == mean(demeaned_ret1)), lty = "dashed", lwd = 4, col="green")
abline(v=which(std_demeaned_ret1 == mean(std_demeaned_ret1)), lty = "dashed", lwd = 3, col="blue")
abline(v=which(normalized_ret1 == mean(normalized_ret1)), lty = "dotted", lwd = 2, col="orange")
```

The orange line shows that we end up with only positive signals.  
We don't want the mean of the scaled signal to be 10. We want the mean of the *absolute* signal to be 10.  


## Scaling signal by mean of *absolute* signal
See pysystemtrade implementation:  
https://github.com/robcarver17/pysystemtrade/blob/master/sysquant/estimators/forecast_scalar.py#L31  
```{r}
set.seed(3746)
sig <- rnorm(1000, 2, 8)
```

This is how pysystemtrade does it:  
Find mean of abs signal  
```{r}
mean_abs_sig <- mean(abs(sig))
mean_abs_sig
```

Standardise signal: Divide signal by mean of absolute signal  
```{r}
std_sig <- sig/mean_abs_sig
```


Get mean of scaled signal  
```{r}
mean(std_sig)
```

Get sd of scaled signal  
```{r}
sd(std_sig)
```

Get mean of positive scaled values  
```{r}
mean(std_sig[std_sig >= 0])
```

Get mean of negative scaled values  
```{r}
mean(std_sig[std_sig < 0])
```

Get sd of positive scaled values  
```{r}
sd(std_sig[std_sig >= 0])
```

Get sd of negative scaled values  
```{r}
sd(std_sig[std_sig < 0])
```

Scale signal to expected absolute value of 10  
```{r}
scaled_sig <- std_sig * 10
```

Mean of the scaled signal  
```{r}
mean(scaled_sig)
```

Mean of the absolute scaled signal  
```{r}
mean(abs(scaled_sig))
```

sd of the scaled signal  
```{r}
sd(scaled_sig)
```

sd of the absolute scaled signal  
```{r}
sd(abs(scaled_sig))
```



Make plot (echo mean in each signal to be able to make vlines)  
```{r, echo=FALSE}
sig1 <- sort(c(sig, mean(sig)))
std_sig1 <- sort(c(std_sig, mean(std_sig)))
scaled_sig1 <- sort(c(scaled_sig, mean(scaled_sig)))

plot(sig1, pch = 16, cex = 0.3, col = "black")
points(std_sig1, pch = 16, cex = 0.3, col = "red")
points(scaled_sig1, pch = 16, cex = 0.3, col = "green")
abline(v=which(sig1 == mean(sig1)), lwd = 3, col="black")
abline(v=which(std_sig1 == mean(std_sig1)), lty = "dashed", lwd = 2, col="red")
abline(v=which(scaled_sig1 == mean(scaled_sig1)), lty = "dotted", lwd = 1, col="green")
```

### mean = 0
Compute the scaling factor for 20 simulated signals with the same input parameters.  
Do this for different values of sd.   
```{r, echo=FALSE}
f_normalization_factor <- function(raw_signal, target = 1) {
  target/mean(abs(raw_signal))
}
```

```{r, echo=FALSE}
sd_vals <- c(1, 0.1, 0.01, 0.001, 0.0001)
factor_vectors <- list()
for(i in seq_along(sd_vals)) {
  factor_vectors[[i]] <- 
    replicate(20, f_normalization_factor(rnorm(10000, 0, sd_vals[i]), 10))
}
range_ <- range(factor_vectors)
```


```{r, echo=FALSE}
plot(factor_vectors[[1]], pch = 16, cex = 0.8, ylim = range_, log = "y", ylab = "scaling_factor", main = "Rescaled signal (lin-log)")
points(factor_vectors[[2]], pch = 16, cex = 0.8, col = "red")
points(factor_vectors[[3]], pch = 16, cex = 0.8, col = "green")
points(factor_vectors[[4]], pch = 16, cex = 0.8, col = "blue")
points(factor_vectors[[5]], pch = 16, cex = 0.8, col = "orange")
```

Means:  
```{r, echo=FALSE}
mean(factor_vectors[[1]])
mean(factor_vectors[[2]])
mean(factor_vectors[[3]])
mean(factor_vectors[[4]])
mean(factor_vectors[[5]])
```
Note: When the mean of the raw signal is close to 0, the scaling factor is inverse proportional to the volatility of the raw signal.  


```{r, echo=FALSE}
mean <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sd <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(mean = mean, sd = sd)
for(i in 1:nrow(grid)) {
  grid[i, "scalar"] <- f_normalization_factor(rnorm(10000, grid$mean[i], grid$sd[i]), 10)
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sd, y = scalar, color = as.factor(mean))) +
  geom_point() +
  geom_line(aes(group = as.factor(mean), color = as.factor(mean))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sd", y = "scalar", color = "mean")
```



```{r, echo=FALSE}
mean <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sd <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(mean = mean, sd = sd)
for(i in 1:nrow(grid)) {
  set.seed(6534)
  grid[i, "mav"] <- mean(abs(rnorm(10000, grid$mean[i], grid$sd[i])))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sd, y = mav, color = as.factor(mean))) +
  geom_point() +
  geom_line(aes(group = as.factor(mean), color = as.factor(mean))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sd", y = "mav", color = "mean")
```



### Diagnosis
What's going on? Let's look at how `mean(abs(x))` depends on `mean(x)`.  

```{r, echo=FALSE}
means <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
signals <- lapply(means, function(m) rnorm(10000, m, 1))
mean_abs_vals <- lapply(signals, function(x) mean(abs(x)))
df <- data.frame(means = means, mean_abs_vals = unlist(mean_abs_vals))
```


```{r, echo=FALSE}
ggplot(df, aes(x = means, y = mean_abs_vals)) +
  geom_point() +
  geom_line() +
  labs(x = "means", y = "mean abs vals")
```

We observe that the mean of the absolute values is equal to the absolute value of the mean:  
```{r}
set.seed(3654)
sgnl1 <- rnorm(10000, -100, 1)
mean(abs(sgnl1)) == abs(mean(sgnl1))
```

But not for all raw signals with a mean close to 0:
```{r}
set.seed(3654)
sgnl2 <- rnorm(10000, 0, 1)
mean(sgnl2)
mean(abs(sgnl2)) == abs(mean(sgnl2))
```

```{r}
set.seed(3654)
sgnl3 <- rnorm(10000, 0.4, 0.1)
set.seed(3654)
sgnl4 <- rnorm(10000, 0.5, 0.1)
mean(abs(sgnl3)) == abs(mean(sgnl3))
mean(abs(sgnl4)) == abs(mean(sgnl4))
```

`mean(abs(sgnl3)) - abs(mean(sgnl3))`  
```{r}
format(
  mean(abs(sgnl3)) - abs(mean(sgnl3)), 
  scientific=F,
  nsmall = 20
)
```

`mean(abs(sgnl4)) - abs(mean(sgnl4))`  
```{r}
format( 
 mean(abs(sgnl4)) - abs(mean(sgnl4)),
 scientific=F,
 nsmall = 20
)
```

Absolute value is also *not* equal to the absolute value of the mean for all values of sd:  
```{r}
set.seed(3654)
sgnl5 <- rnorm(10000, 1, 0.1)
set.seed(3654)
sgnl6 <- rnorm(10000, 1, 0.2)
set.seed(3654)
sgnl7 <- rnorm(10000, 1, 0.3)
set.seed(3654)
sgnl8 <- rnorm(10000, 1, 1)
mean(abs(sgnl5)) == abs(mean(sgnl5))
mean(abs(sgnl6)) == abs(mean(sgnl6))
mean(abs(sgnl7)) == abs(mean(sgnl7))
mean(abs(sgnl8)) == abs(mean(sgnl8))
```



```{r, echo=FALSE}
mean <- c(-10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10)
sd <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(mean = mean, sd = sd)
for(i in 1:nrow(grid)) {
  signal <- rnorm(10000, grid$mean[i], grid$sd[i])
  grid[i, "mean_abs_vals"] <- mean(abs(signal))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = mean, y = mean_abs_vals, color = as.factor(sd))) +
  geom_point() +
  geom_line(aes(group = as.factor(sd), color = as.factor(sd))) +
  labs(x = "mean", y = "mean_abs_val", color = "sd") +
  labs(title = "lin-lin")
```

```{r, echo=FALSE}
ggplot(grid, aes(x = mean, y = mean_abs_vals, color = as.factor(sd))) +
  geom_point() +
  geom_line(aes(group = as.factor(sd), color = as.factor(sd))) +
  scale_y_log10() +
  labs(x = "mean", y = "mean_abs_val", color = "sd") +
  labs(title = "lin-log")
```

#### Analysis

The bigger the sd,  

* the bigger `mean(abs(signal)) - abs(mean(signal))`.   
* $\Longrightarrow$ the smaller the *scaling factor*.  
* the less sensitive the scaling factor is to the mean of the raw signal.  

The smaller the sd,  

* the the smaller `mean(abs(signal)) - abs(mean(signal))`.   
* $\Longrightarrow$ the bigger the *scaling factor*.   
* $\Longrightarrow$ the bigger the scaled signal.  
* the more sensitive the scaling factor is to the mean of the raw signal.  

When the mean of the raw signal is *not* close to zero:  

* `mean(abs(signal))` and `abs(mean(signal))` are effectively equal for *small* sd.  

When the mean of the raw signal is 0,  

* the *scaling factor* is inverse proportional to sd.  

The bigger the absolute mean of the signal,  

* the less sensitive the *scaling factor* is to sd.  

Notice:

* Doubling the <u>standard deviation</u> of a mean zero Gaussian will also double it's <u>average absolute value</u>.  
* The *expected absolute value* of a Gaussian is equal to `sd * sqrt(2/pi)`.  
* If the mean isn't zero (which for slow trend following and carry is very likely) then the standard deviation will be biased downwards compared to abs() although that would be easy to fix by using MAD rather than STDEV.  
* These points are from this discussion:  
  * https://qoppac.blogspot.com/2016/01/pysystemtrader-estimated-forecast.html  
  * (CarverRobert_SystematicTrading#F6: Price Volatility as Exponentially Weighted Moving Average (EWMA))  

### mean = 1
```{r, echo=FALSE}
sd_vals <- c(1, 0.1, 0.01, 0.001, 0.0001)
factor_vectors <- list()
for(i in seq_along(sd_vals)) {
  factor_vectors[[i]] <- 
    replicate(20, f_normalization_factor(rnorm(10000, 1, sd_vals[i]), 10))
}
range_ <- range(factor_vectors)
```


lin-lin
```{r, echo=FALSE}
plot(factor_vectors[[1]], pch = 16, cex = 0.8, ylim = range_, ylab = "scaling_factor",
    main= "Rescaled signal (lin-lin)")
points(factor_vectors[[2]], pch = 16, cex = 0.8, col = "red")
points(factor_vectors[[3]], pch = 16, cex = 0.8, col = "green")
points(factor_vectors[[4]], pch = 16, cex = 0.8, col = "blue")
points(factor_vectors[[5]], pch = 16, cex = 0.8, col = "orange")
```

Remove the signal with sd=1
```{r, echo=FALSE}
range_ <- range(factor_vectors[2:5])
plot(factor_vectors[[2]], pch = 16, cex = 0.8, ylim = range_, ylab = "scaling_factor",
    main= "Rescaled signal (lin-lin)", col = "red")
#points(factor_vectors[[2]], pch = 16, cex = 0.8, col = "red")
points(factor_vectors[[3]], pch = 16, cex = 0.8, col = "green")
points(factor_vectors[[4]], pch = 16, cex = 0.8, col = "blue")
points(factor_vectors[[5]], pch = 16, cex = 0.8, col = "orange")
```

Means 
```{r, echo=FALSE}
mean(factor_vectors[[1]])
mean(factor_vectors[[2]])
mean(factor_vectors[[3]])
mean(factor_vectors[[4]])
mean(factor_vectors[[5]])
```

This looks much more reasonable!



## Diversification Multipliers

+ Notes: ST, Ch 10: Position sizing:  
	+ Note: When the expected absolute value of the individual forecasts is 10, the *diversification multiplier* will ensure that the combined forecast is also 10.  
		+ *** #todo Is this true?? ***  
			+ See Noter: ST, Ch 8: Combined forecasts  

### Functions
```{r}
f_sig_div_mult <- function(
    signal_correlations,
    signal_weights,
    min_cor = 0,
    max_sdm = 2.5) {
  H <- signal_correlations
  w <- signal_weights

  clamped_H <- clamp_matrix_lower(H, min_cor)
  clamp_signal_upper(
    1/sqrt(crossprod(t(w %*% clamped_H),  w)),
    max_signal = max_sdm
  )
}

f_signal_cor_mat <- function(
    signals
) {
  cor(signals)
}

f_signal_cov_mat <- function(
    signals
) {
  cov(signals)
}

f_normalization_factor <- function(raw_signal, target = 1) {
  target/mean(abs(raw_signal))
}

f_normalize_signal <- function(signal, normalization_factor = 1) {
  signal * normalization_factor
}

clamp_signal <- function(signal, min_signal = -Inf, max_signal = Inf) {
  #if(length(signal) != 1) {stop("clamp_signal() only takes a single number
  #                              as input.")}
  vapply(
    signal,
    function(x) {max(min_signal, min(x, max_signal))},
    numeric(1)
  )
}

clamp_matrix_lower <- function(input_matrix, min_signal) {
  apply(input_matrix,
        c(1,2),
        clamp_signal_lower,
        min_signal = min_signal
  )
}

clamp_signal_lower <- function(signal, min_signal = -Inf) {
  max(min_signal, signal)
}

clamp_signal_upper <- function(signal, max_signal = Inf) {
  min(signal, max_signal)
}

## This function is modified, so we don't have to load from a system list.
## Only one instrument
combine_signals <- function(
    clamped_signals,
    signal_weights,
    min_cor = 0,
    max_sdm = 2.5,
    sdm_mode = "div_mult",
    cor_or_cov = "cor") {

    ## Calculate signal correlations.
    ## No need to do this every time we run update_position_table_row.
    clamped_signals_mat <- sapply(clamped_signals, function(x) x)
    
    if(cor_or_cov == "cor") {
      signal_cor_mat <- f_signal_cor_mat(clamped_signals_mat)
    } else if (cor_or_cov == "cov") {
      signal_cor_mat <- f_signal_cov_mat(clamped_signals_mat)
    }
      
    combined_signals <- clamped_signals_mat %*% signal_weights
    
    ## Signal Diversification Multiplier
    if(sdm_mode == "div_mult") {
      sdm <- f_sig_div_mult(
        signal_cor_mat, 
        signal_weights
      )
      print(paste("sdm: ", sdm))
    } else if(sdm_mode == "var") {
      n <- length(combined_signals)
      bias_correction <- sqrt((n - 1)/n)
      sdm <- 1/(sd(combined_signals) * bias_correction)
      print(paste("sdm: ", sdm))
    }
    combined_signals <-  combined_signals * sdm
    
  #}
  combined_signals
}
```

### Calculations
Generate 5 signals
```{r}
set.seed(876234)
signals <- lapply(1:5, function(x) rnorm(200, 0, 1))
```

Calculate normalization factors
```{r}
norm_factors <- lapply(signals, function(x) {f_normalization_factor(x, target = 10)})
norm_factors
```

Normalize signals  
Show mean of absolute normalized signals (target = 10)
```{r}
normalized_signals <- list()
for(i in seq_along(signals)) {
  normalized_signals[[i]] <- f_normalize_signal(signals[[i]], norm_factors[[i]])
}

lapply(normalized_signals, function(x) mean(abs(x)))
```

Clamped signals  
Show mean of absolute clamped signals (target = 10)
```{r}
clamped_signals <- lapply(
  normalized_signals,
  function(x) {clamp_signal(x, min_signal = -20, max_signal = 20)}
)

lapply(clamped_signals, function(x) mean(abs(x)))
```
Observe:  
Because we clamped the signal, the mean abs no longer hits the target.  
This is ok, but it means that the rescaled combined signal will also not meet
the target.  



Combine signals.   
Show mean absolute value of combined signal.
```{r}
signal_weights <- rep(1/5, 5)
combined_signal <- combine_signals(
  clamped_signals,
  #normalized_signals,
  signal_weights
)

mean(abs(combined_signal))
```



### Conclusion

The mean absolute combined signal is a bit below the target, because of the clamping.



### Test function

```{r}
div_test_funk <- function(
    m, 
    s,
    min_signal = -20,
    max_signal = 20,
    min_cor = 0,
    max_sdm = 2.5,
    sdm_mode = "div_mult",
    cor_or_cov = "cor"
    ) {
  signals <- lapply(1:5, function(x) rnorm(200, m, s))
  norm_factors <- lapply(signals, function(x) {f_normalization_factor(x, target = 10)})
  normalized_signals <- list()
  for(i in seq_along(signals)) {
    normalized_signals[[i]] <- f_normalize_signal(signals[[i]], norm_factors[[i]])
  }
  clamped_signals <- lapply(
    normalized_signals,
    function(x) {clamp_signal(x, min_signal = -20, max_signal = 20)}
  )
  signal_weights <- rep(1/5, 5)
  combined_signal <- combine_signals(
    clamped_signals,
    #normalized_signals,
    signal_weights,
    sdm_mode = sdm_mode,
    min_cor = min_cor,
    max_sdm = max_sdm,
    cor_or_cov = cor_or_cov
  )
  list(signals = signals, 
   norm_factors = norm_factors, 
   normalized_signals = normalized_signals, 
   clamped_signals = clamped_signals, 
   signal_weights = signal_weights, 
   combined_signal = combined_signal
  )
}
```


```{r}
div_test <- div_test_funk(
  m = 0, 
  s = 1,
  min_signal = -20,
  max_signal = 20,
  min_cor = 0,
  max_sdm = 2.5
)

mean(abs(div_test$combined_signal))
```

```{r}
div_test <- div_test_funk(
  m = 0, 
  s = 1,
  min_signal = -Inf,
  max_signal = Inf,
  min_cor = -Inf,
  max_sdm = Inf
)

mean(abs(div_test$combined_signal))
```

Observation:  
Removing the restrictions doesn't get us closer to the target.  

Let's try different mean and sd of the raw signal:
```{r}
div_test <- div_test_funk(
  m = -1, 
  s = 4,
  min_signal = -Inf,
  max_signal = Inf,
  min_cor = -Inf,
  max_sdm = Inf
)

mean(abs(div_test$combined_signal))
```

Observation:  
So this phenomenon has to do with the relation between the mean(signal) and mean(abs(signal)).



Compare with 1/sd() instead of 1/(w^THw)
```{r}
div_test <- div_test_funk(
  m = -1, 
  s = 4,
  min_signal = -Inf,
  max_signal = Inf,
  min_cor = -Inf,
  max_sdm = Inf,
  sdm_mode = "var"
)

mean(abs(div_test$combined_signal))
```
```{r}
head(div_test$combined_signal)
```
```{r}
div_test <- div_test_funk(
  m = -1, 
  s = 4,
  min_signal = -Inf,
  max_signal = Inf,
  min_cor = -Inf,
  max_sdm = Inf,
  sdm_mode = "div_mult",
  cor_or_cov = "cor"
)

mean(abs(div_test$combined_signal))
```

```{r}
div_test <- div_test_funk(
  m = 0.1, 
  s = 0.1,
  min_signal = -Inf,
  max_signal = Inf,
  min_cor = -Inf,
  max_sdm = Inf,
  sdm_mode = "div_mult", #"div_mult" or "var"
  cor_or_cov = "cor" #"cor" or "cov"
)

mean(abs(div_test$combined_signal))
```



### Half-normal distribution and Mean Absolute Value
The expected absolute value of a normal random variable with mean $\mu=0$ and standard deviation $\sigma$ is $\sigma \sqrt{\frac{2}{\pi}}$.  
See Half-normal distribution:   
https://en.wikipedia.org/wiki/Half-normal_distribution  
```{r}
std_norm_signal <- rnorm(1000000, 0, 1)

sd_pop <- function(x) {
  n <- length(x)
  bias_factor <- (n - 1) / n
  sd(x) * bias_factor
}

sqrt(2/pi) * sd_pop(std_norm_signal)
mean(abs(std_norm_signal))
```


How does the absolute difference between the two methods depend on the number of observations?
```{r, echo=FALSE}
nnn <- 2^(4:23)
signals <- lapply(nnn, function(n) {
  set.seed(76523)
  rnorm(n, 0, 1)
})
aaa <- vapply(signals, function(x) {sqrt(2/pi) * sd_pop(x)}, numeric(1))
bbb <- vapply(signals, function(x) {mean(abs(x))}, numeric(1))
diffs <- abs(aaa - bbb)
df <- data.frame(nnn = nnn, diffs = diffs)
```

```{r, echo=FALSE}
ggplot(df, aes(x = nnn, y = diffs)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(trans='log2')  +
  scale_y_continuous(trans='log2')  +
  labs(x = "n", y = "diff") + 
  labs(title = "log-log")
```








## SDM
Signal multiplication multiplier

```{r}
signal_1 <- round(rnorm(1000), 2)
signal_2 <- round(rnorm(1000), 2)
```

```{r}
signals <- data.frame(
  x1 = signal_1,
  x2 = signal_2
)
```

```{r}
cov(signal_1, signal_2)
cov(signals)
cov(signal_1, signal_1)
```

Convert from correlation matrix to covariance matrix:  
Multiply cor matrix on both sides by diagonal matrix with sd's in diagonal.  
```{r}
D <- diag(c(sd(signal_1), sd(signal_2)))
D %*% cor(signals) %*% D

## compare:
cov(signals)
```

Convert from covariance matrix to correlation matrix:  
Multiply cov matrix on both sides by diagonal matrix with inverse sd's in diagonal.  
Get sd's from diagonals in cov matrix.  
Note: `diag(<vector>)` produces a diagonal matrix. `diag(<matrix>)` produces a vector of the diagonal.  
```{r}
D_inv <- solve(
  sqrt(
    diag(
      diag(
        cov(signals)
      )
    )
  )
) ## Inverse
D_inv %*% cov(signals) %*% D_inv

## compare:
V <- diag(cov(signals)) ## Get variances from cov matrix

cov(signals)/V

## compare:
cor(signals)
```

```{r}
H <- cor(signals)
V <- cov(signals)
signals_mat <- as.matrix(signals)
W <- c(0.3, 0.7)
comb_signals <- signals_mat %*% W ## X_hat
```

Check that $\sigma[\hat{X}] \cdot K = \text{MAV} [\hat{X}]$.  
When $\text{mean}(\hat{X})=0$, $K = \sqrt{2/\pi}$.  
Here, the mean of $\hat{X}$ is not zero, so we shift $\hat{X}$ to get a zero mean.   
Now the expected difference is zero.  
0.01167052 is close to zero. Close enough?  
```{r}
## 1)
K <- sqrt(2/pi)
## Shift to zero mean
comb_sig_demean <- comb_signals - mean(comb_signals)
## The expected difference is zero
(sd(comb_sig_demean) * K) - mean(abs(comb_sig_demean))
```

These should be identical
```{r}
## 2a)
sqrt(var(comb_signals))[1,1]

## 2b)
sd(comb_signals)

## 3)
sqrt(
  crossprod(
    t(W %*% V),  
    W
  )
)[1,1]

## 4)
Ws <- W * (diag(sqrt(V))) ## weighted st. devs
sqrt(
  crossprod(
    t(Ws %*% H),  
    Ws
  )
)[1,1]

```
Observations:  
* As expected, we have a conversion from $W^THW$ to $W^TVW$ in 1).  
* As expected, $WCW$ is equal to $V[X]$, where $X$ is a vector of the two stochastic variables generating the two signals.  









```{r}
H <- cov(signals)
W <- c(0.5, 0.5)
```

```{r}
f_sig_div_mult(H, W)
```

```{r}
1/(sd(as.matrix(signals) %*% W))
```

```{r}
signals_mat <- as.matrix(signals)
W <- c(0.3, 0.7)
comb_signals <- signals_mat %*% W
```

Calculate combination of first elements in each signal manually and compare with `comb_signals`.
```{r}
0.3 * signals[1, 1] + 0.7 * signals[1, 2]
comb_signals[1]
```

Compare manual calculation based on correlations with sd(comb_signals)
```{r}
sqrt(crossprod(t(W %*% cov(signals)),  W))
sqrt(var(comb_signals))
sd(comb_signals)
```



```{r}
clamp_matrix_lower <- function(input_matrix, min_signal) {
  apply(input_matrix,
        c(1,2),
        clamp_signal_lower,
        min_signal = min_signal
  )
}
```


```{r}
clamp_matrix_lower(H, 0.5)
```

```{r}
clamp_matrix_lower(H, min_signal = 0)
H
```



## Comparison 1: Compare standard deviation and MAV

For different distributions of $X_i \in \{{X_i}\}_{i=1}^p$, compute  

$$K = \frac{\text{MAV}[\mathbf{\hat{X}}]}{\sigma_{\hat{X}}}$$

- wrt. $\mu$  
- wrt $\sigma$
		
where  
- $X_i$: A stochastic variable. Produces a single signal vector.  
- $\mathbf{\tilde{X}}$: A $(p \times 1)$ weighted stochastic vector:  

$$\mathbf{\tilde{X}} := \mathbf{w} \circ \mathbf{X} \equiv [w_1 X_1, w_2 X_2, \ldots, w_p X_p]^T$$

- $\hat{X} := \sum_i^p \tilde{X}_i$: Combined, weighted signal. Stochastic variable.  

### Signal normal distributed

p is number of rules. So p is realistically quite low. Here it's 3.  

```{r, eval=FALSE}
p <- 3L
n <- 10000
m <- 0 
sigma <- 1
num_tests <- 100
```
```{r, echo=FALSE}
p <- 3L
n <- 10000L ## Number of observations, length of signal.
            ## 10000 daily observations is approx 40 years.
m <- 0 ## Population mean of the individual signals in X
sigma <- 1 ## Population sd of the individual signals in X

num_tests <- 100

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_norm_X <- function(n, p, m, sigma) {
  X <- matrix(rnorm(n * p, m, sigma), nrow = n)
  X
}

make_X_hat <- function(X) {
  X_hat <- numeric(nrow(X))
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

mav <- function(x) {mean(abs(x))}

## Generate X_hat num_test times.
norm_test_data <- lapply(1:num_tests, function(.x) {
  make_X_hat(
    make_norm_X(n, p, m, sigma) 
  )
})

## For each test, get the standard deviation of the X_hat signal, i.e. the combined signal.
norm_test <- data.frame(unlist(lapply(norm_test_data, sd)))
names(norm_test) <- "sd"

## For each test, get the MAV of the X_hat signal, i.e. the combined signal.
norm_test$mav <- unlist(lapply(norm_test_data, mav))
norm_test$K <- norm_test$mav / norm_test$sd

norm_test <- arrange(norm_test, sd)
norm_test$ID <- as.numeric(row.names(norm_test))
norm_test <- norm_test[c(4, 1, 2, 3)]


plot_data <- norm_test %>% gather(key = "variable", value = "value", -c(ID, K)) 

ggplot(plot_data, aes(x = ID, y = value)) +
  geom_line(aes(color = variable)) +
  labs(title = "lin-lin")
```

The red line is $f(x) = x$.  
MAV tends to lie below sd.
```{r, echo=FALSE}
ggplot(norm_test, aes(x = sd, y = mav)) +
  geom_line() +
  geom_function(fun = function(x) x, color = "red")
```

How does K depend on the population mean and standard deviation of the individual signals?  
Assume that the distribution for all individual signals is the same.
```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

mav <- function(x) {mean(abs(x))}

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_norm_X <- function(n, p, m, sigma) {
  X <- matrix(rnorm(n * p, m, sigma), nrow = n)
  X
}

make_X_hat <- function(X) {
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  X_hat <- make_X_hat(make_norm_X(n, p, grid$m[i], grid$sigma[i]))
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i]/grid$sd[i]
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = K, color = as.factor(m))) +
  geom_point() +
  geom_line(aes(group = as.factor(m), color = as.factor(m))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "K", color = "m") +
  labs(title = "log-log")
```

For now it looks like:  
- K is close to 1 if (e.g):  
    - $\sigma > 0.01$ and $0 < \text{abs}(m)  < 0.001$.  
    - $\sigma > 1$ and $0 < \text{abs}(m) < 0.1$.  
- K close to 1 means that MAV and sd are equivalent. 


How close is K to 1, when $\mu = 0$?
```{r}
range(grid$K[grid$m == 0])
```
So:  
- When $\mu = 0$, then $K$ is between $0.7172$ and $0.7418$.  

This raises the question:  
- For which $\mu$ is $K = 1$?  

```{r, echo=FALSE}
#sigma <- c(10000, 1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
sigma <- 2^(c(-10:10))

ndeps <- 1e-06

seed <- 6354


obj_funct <- function(mu, sigma) {
  p <- 3L
  n <- 10000L ## number of observations, length of signal
  #sigma = 0.1
  mav <- function(x) {mean(abs(x))}
  
  ## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
  ## X_hat is the combined signal, here using equal weights.
  make_norm_X <- function(n, p, mu, sigma) {
    X <- matrix(rnorm(n * p, mu, sigma), nrow = n)
    X
  }
  
  make_X_hat <- function(X) {
    X_hat <- numeric(n)
    for(i in 1:n){X_hat[i] <-mean(X[i, ])}
    X_hat
  }
  
  X_hat <- make_X_hat(make_norm_X(n, p, mu, sigma))
  s <- sd(X_hat)
  mav <- mav(X_hat)
  K <- mav / s
  
  ## optim() is a minimizer.
  ## This function takes minimum, when K = 1.
  (K - 1)^2
}

obj_grad <- function(mu, sigma) {
  p <- 3L
  n <- 10000L ## number of observations, length of signal
  #sigma = 0.1
  mav <- function(x) {mean(abs(x))}
  
  ## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
  ## X_hat is the combined signal, here using equal weights.
  make_norm_X <- function(n, p, mu, sigma) {
    X <- matrix(rnorm(n * p, mu, sigma), nrow = n)
    X
  }
  
  make_X_hat <- function(X) {
    n <- nrow(X)
    X_hat <- numeric(n)
    for(i in 1:n){X_hat[i] <-mean(X[i, ])}
    X_hat
  }
  
  X_hat <- make_X_hat(make_norm_X(n, p, mu, sigma))
  s <- sd(X_hat)
  mav <- mav(X_hat)
  K <- mav / s
  
  ## optim() is a minimizer.
  ## This is the gradient of the object function
  2*K - 2
}


vals = numeric(length(sigma))
for(i in seq_along(sigma)) {
  set.seed(seed)
  vals[i] <- optim(0.5, obj_funct, obj_grad, sigma = sigma[i], method = "L-BFGS-B", lower = 0, upper = 1, control = list(ndeps = ndeps))$value
}
```

```{r, echo=FALSE}
plot_data <- data.frame(sigma = sigma, vals = vals)

ggplot(plot_data, aes(x = sigma, y = vals)) +
  geom_point() +
  geom_line() +
  #scale_x_log10() +
 scale_x_continuous(
   trans = "log2", 
   breaks = 10^(-3:3),# sigma,
   labels = scales::math_format(10^.x, format = log10)) +# scales::math_format(2^.x, format = log2)) +
  #scale_y_log10() +
  scale_y_continuous(
    trans = "log2",
    breaks = 10^(-7:0), #2^((-10:1)*2),
    labels = scales::math_format(10^.x, format = log10)) +# scales::math_format(2^.x, format = log2)) +
  labs(x = "sigma", y = "optimal mu") +
  labs(title = "log-log")
```

Range of optimal values of $\mu$:
```{r}
range(vals)
```


Inspect the mid range.  
```{r, echo=FALSE}
#sigma <- c(10000, 1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
sigma <- (1:50)/20

ndeps <- 1e-06

seed <- 6354


obj_funct <- function(mu, sigma) {
  p <- 3L
  n <- 10000L ## number of observations, length of signal
  #sigma = 0.1
  mav <- function(x) {mean(abs(x))}
  
  ## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
  ## X_hat is the combined signal, here using equal weights.
  make_norm_X <- function(n, p, mu, sigma) {
    X <- matrix(rnorm(n * p, mu, sigma), nrow = n)
    X
  }
  
  make_X_hat <- function(X) {
    X_hat <- numeric(n)
    for(i in 1:n){X_hat[i] <-mean(X[i, ])}
    X_hat
  }
  
  X_hat <- make_X_hat(make_norm_X(n, p, mu, sigma))
  s <- sd(X_hat)
  mav <- mav(X_hat)
  K <- mav / s
  
  ## optim() is a minimizer.
  ## This function takes minimum, when K = 1.
  (K - 1)^2
}

obj_grad <- function(mu, sigma) {
  p <- 3L
  n <- 10000L ## number of observations, length of signal
  #sigma = 0.1
  mav <- function(x) {mean(abs(x))}
  
  ## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
  ## X_hat is the combined signal, here using equal weights.
  make_norm_X <- function(n, p, mu, sigma) {
    X <- matrix(rnorm(n * p, mu, sigma), nrow = n)
    X
  }
  
  make_X_hat <- function(X) {
    n <- nrow(X)
    X_hat <- numeric(n)
    for(i in 1:n){X_hat[i] <-mean(X[i, ])}
    X_hat
  }
  
  X_hat <- make_X_hat(make_norm_X(n, p, mu, sigma))
  s <- sd(X_hat)
  mav <- mav(X_hat)
  K <- mav / s
  
  ## optim() is a minimizer.
  ## This is the gradient of the object function
  2*K - 2
}


vals = numeric(length(sigma))
for(i in seq_along(sigma)) {
  set.seed(seed)
  vals[i] <- optim(0.5, obj_funct, obj_grad, sigma = sigma[i], method = "L-BFGS-B", lower = 0, upper = 1, control = list(ndeps = ndeps))$value
}
```

```{r, echo=FALSE}
plot_data <- data.frame(sigma = sigma, vals = vals)

ggplot(plot_data, aes(x = sigma, y = vals)) +
  geom_point() +
  geom_line() +
  #scale_x_log10() +
# scale_x_continuous(
#   trans = "log2", 
#   breaks = sigma,
#   labels = scales::math_format(2^.x, format = log2)) +
  #scale_y_log10() +
  scale_y_continuous(
    trans = "log2",
    breaks = 10^(-7:0), #2^((-10:1)*2),
    labels = scales::math_format(10^.x, format = log10)) +# scales::math_format(2^.x, format = log2)) +
  labs(x = "sigma", y = "optimal mu") +
  labs(title = "lin-log")
```



#### Conclusion

- K is close to 1 if (e.g):  
    - $\sigma > 0.01$ and $0 < \text{abs}(m)  < 0.001$.  
    - $\sigma > 1$ and $0 < \text{abs}(m) < 0.1$.  
- K close to 1 means that MAV and sd are equivalent.  
- WARNING: K can be huge, if the mean of the individual signals is different from 0, and the standard deviation is close to 0.  
- The optimal $\mu$ is quite unstable wrt. $\sigma$ for different draws of $X$.  
  - Especially in the range $\sigma \in [0.01, 4]$.  
    - This means that anywhere near a standard normal distribution, $K$ is unstable.  
- If $\sigma$ is large ($\geq 10$), then optimal $\mu$ seems stable, and therefore $K$ should also be stable wrt. $\sigma$.  
- For $\sigma \geq 10$ the optimal $\mu$ seems to be around $0.04$.  


### Signals follow a skewed t-distribution

Generate a skewed t-distributed series and inspect.  

```{r, eval = FALSE}
m = 0
s = 1
nu = 5
xi = 0.4
n = 1000
```


```{r, echo=FALSE}
m = 0
s = 1
nu = 5
xi = 0.4
n = 1000
x <- rsstd(n, mean = m, sd = s, nu = nu, xi = xi)
plot(x, type = "l", main = "sstd", col = "steelblue")
curve(dsstd(x, mean = m, sd = s, nu = nu, xi = xi), add=TRUE, lty="dotted")
```

```{r}
hist(x, n = 50)
```

How does K depend on the population mean and standard deviation of the individual signals?  
Assume that the distribution for all individual signals is the same.
```{r, eval=FALSE}
p = 3
n = 10000
m = 0
s = 1
nu = 5
xi = 0.4
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal
nu <- 5
xi <- 0.4

m <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

mav <- function(x) {mean(abs(x))}

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_sstd_X <- function(n, p, m, sigma, nu, xi) {
  X <- matrix(rsstd(n, mean = m, sd = sigma, nu = nu, xi = xi), nrow = n)
  X
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  X_hat <- make_X_hat(make_sstd_X(n, p, grid$m[i], grid$sigma[i], nu, xi))
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i]/grid$sd[i]
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = K, color = as.factor(m))) +
  geom_point() +
  geom_line(aes(group = as.factor(m), color = as.factor(m))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "K", color = "m") +
  labs(title = "log-log")
```

#### Conclusion

We see a similar pattern as with normal distributed signals, with the difference that K differs a bit for opposite signed values of $\mu$.  

By eye it looks like K actually tends to be bigger for normal distributed signals than for heavily skewed signals.  


### Signals follow different t-distributions

How does K depend on the population mean and standard deviation of the individual signals?  

The signals are distributed by all permutations of the following parameter values:  
```{r, eval=FALSE}
p <- 3
n <- 10000
m <- c(-10, -0.1, 0, 0.1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001)
nu <- c(3, 30)
xi <- c(0.2, 0.5, 0.8)
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-10, -0.1, 0, 0.1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001)
nu <- c(3, 30)
xi <- c(0.2, 0.5, 0.8)
grid <- expand.grid(m = m, sigma = sigma, nu = nu, xi = xi)

seed <- 6354

mav <- function(x) {mean(abs(x))}

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_sstd_X <- function(n, p, m, sigma, nu, xi) {
  X <- matrix(rsstd(n, mean = m, sd = sigma, nu = nu, xi = xi), nrow = n)
  X
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  set.seed(seed)
  X_hat <- make_X_hat(make_sstd_X(n, p, grid$m[i], grid$sigma[i], grid$nu[i], grid$xi[i]))
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i]/grid$sd[i]
  grid[i, "grp"] <- paste0(as.character(grid$m[i]), as.character(grid$nu[i]), as.character(grid$xi[i]))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = K, color = as.factor(m))) +
  geom_point() +
  geom_line(aes(group = as.factor(grp), color = as.factor(m))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "K", color = "m") +
  labs(title = "log-log")
```



#### Conclusion

- $K$ doesn't look dramatically different when idd for mixed distributions by visual inspection.



## Comparison 2

Study to which degree the contributions of $\mathbf{D}$ and $K$ cancel each other out.    

$$\frac{\sqrt{\mathbf{w}^T \mathbf{D} \mathbf{H} \mathbf{D} \mathbf{w}} \cdot K}{\sqrt{\mathbf{w}^T \mathbf{H} \mathbf{w}}} = 1 \text{?}$$

If the contributions from $\mathbf{D}$ and $K$ cancel each other out, we have:  

$$= \left(\sum_i^p \tilde{X}_i \right) \frac{\text{MAV}_{\tau}}{\sqrt{\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}} \cdot K}$$

$$= \left(\sum_i^p \tilde{X}_i \right)\frac{\text{MAV}_{\tau}}{\sqrt{\mathbf{w}^T \mathbf{D} \mathbf{H} \mathbf{D} \mathbf{w}} \cdot K}$$

$$= \left(\sum_i^p \tilde{X}_i \right)\frac{\text{MAV}_{\tau}}{\sqrt{\mathbf{w}^T \mathbf{H} \mathbf{w}}}$$

...which would mean, that the two methods are indeed identical.


### Signal normal distributed

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

w <- rep(1/p, p)

mav <- function(x) {mean(abs(x))}

make_norm_X <- function(n, p, m, sigma) {
  matrix(rnorm(n * p, m, sigma), nrow = n)
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

make_multipliers <- function(n, p, m, sigma, w) {
  X <- make_norm_X(n, p, m, sigma)
  X_hat <- make_X_hat(X)
  Sigma <- cov(X)
  H <- cor(X)
  D <- sqrt(diag(diag(Sigma)))

  A1 <- sqrt(crossprod(t(w %*% D %*% H %*% D),  w))
  A2 <- sqrt(crossprod(t(w %*% H), w))
  
  list(A1, A2)
}

for(i in 1:nrow(grid)) {
  X_hat <- make_X_hat(make_norm_X(n, p, grid$m[i], grid$sigma[i]))
  mult <- make_multipliers(n, p, grid$m[i], grid$sigma[i], w)
  grid[i, "A1"] <- mult[1]
  grid[i, "A2"] <- mult[2]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i] / grid$sd[i]
  grid[i, "frac"] <- (grid$A1[i] * grid$K[i]) / grid$A2[i]
  grid[i, "grp"] <- paste0(as.character(grid$m[i]), as.character(grid$nu[i]), as.character(grid$xi[i]))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = frac, color = as.factor(m))) +
  geom_point() +
  geom_line(aes(group = as.factor(grp), color = as.factor(m))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "frac", color = "m")
```

#### Conclusion

- The conclusion for normal distributed signals is analogous to the one in comparison 1 above.  
- The contributions of $\mathbf{D}$ and $K$ cancel each other out, if (e.g.):  
    - $\sigma > 0.01$ and $0 < \text{abs}(m)  < 0.001$.  
    - $\sigma > 1$ and $\text{abs}(m) < 0.1$.  
- WARNING: The difference can be huge, if the mean of the individual signals is different from 0, and the standard deviation is close to 0.  
- If we use $w^THw$ instead of $w^T\Sigma w$, the diversification multiplier can be many (millions of) orders of magnitude bigger than the theoretical MAV that we are targeting, oi $\sigma$ is small and $\mu$ is not $0$.
- IMPORTANT: For these reasons, the target mean average signal should be bigger than 1. 10 seems good, or even 100, which may also be more intuitive (200 means double, as in 200 percent).


### Signals follow a skewed t-distribution

```{r, eval=FALSE}
p = 3
n = 10000
nu = 5
xi = 0.4
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal
nu <- 5
xi <- 0.4

m <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

w <- rep(1/p, p)

mav <- function(x) {mean(abs(x))}

make_sstd_X <- function(n, p, m, sigma, nu, xi) {
  X <- matrix(rsstd(n * p, mean = m, sd = sigma, nu = nu, xi = xi), nrow = n)
  X
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <- mean(X[i, ])}
  X_hat
}

make_multipliers <- function(n, p, m, sigma, nu, xi, w) {
  X <- make_sstd_X(n, p, m, sigma, nu, xi)
  X_hat <- make_X_hat(X)
  Sigma <- cov(X)
  H <- cor(X)
  D <- sqrt(diag(diag(Sigma)))

  A1 <- sqrt(crossprod(t(w %*% D %*% H %*% D),  w))
  A2 <- sqrt(crossprod(t(w %*% H), w))
  
  list(A1, A2)
}

for(i in 1:nrow(grid)) {
  X_hat <- make_X_hat(make_sstd_X(n, p, grid$m[i], grid$sigma[i], nu, xi))
  mult <- make_multipliers(n, p, grid$m[i], grid$sigma[i], nu, xi, w)
  grid[i, "A1"] <- mult[1]
  grid[i, "A2"] <- mult[2]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i] / grid$sd[i]
  grid[i, "frac"] <- (grid$A1[i] * grid$K[i]) / grid$A2[i]
  grid[i, "grp"] <- paste0(as.character(grid$m[i]), as.character(grid$nu[i]), as.character(grid$xi[i]))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = frac, color = as.factor(m))) +
  geom_point() +
  geom_line(aes(group = as.factor(grp), color = as.factor(m))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "frac", color = "m")
```


#### Conclusion

- For the heavily skewed signals, the picture is quite similar to the normal distributed data.  
- For $\mu = 0$, the fraction is approximately equal to $\sigma$.  
- The further $\mu$ is from $0$, the bigger the fraction.  
- From visual inspection: The fraction is quite stable around $1$ when $\mu$ is somewhere between $0.1$ and $1$, and $\sigma < 1$.  


### Signals follow different t-distributions
The signals are distributed by all permutations of the following parameter values:  
```{r, eval=FALSE}
p = 3
n = 10000
m <- c(-10, -0.1, 0, 0.1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001)
nu <- c(3, 30)
xi <- c(0.2, 0.5, 0.8)
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-10, -0.1, 0, 0.1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001)
nu <- c(3, 30)
xi <- c(0.2, 0.5, 0.8)
grid <- expand.grid(m = m, sigma = sigma, nu = nu, xi = xi)

seed <- 6354

w <- rep(1/p, p)

mav <- function(x) {mean(abs(x))}

make_sstd_X <- function(n, p, m, sigma, nu, xi) {
  X <- matrix(rsstd(n * p, mean = m, sd = sigma, nu = nu, xi = xi), nrow = n)
  X
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <- mean(X[i, ])}
  X_hat
}

make_multipliers <- function(n, p, m, sigma, nu, xi, w) {
  X <- make_sstd_X(n, p, m, sigma, nu, xi)
  X_hat <- make_X_hat(X)
  Sigma <- cov(X)
  H <- cor(X)
  D <- sqrt(diag(diag(Sigma)))

  A1 <- sqrt(crossprod(t(w %*% D %*% H %*% D),  w))
  A2 <- sqrt(crossprod(t(w %*% H), w))
  
  list(A1, A2)
}

for(i in 1:nrow(grid)) {
  set.seed(seed)
  X_hat <- make_X_hat(make_sstd_X(n, p, grid$m[i], grid$sigma[i], grid$nu[i], grid$xi[i]))
  mult <- make_multipliers(n, p, grid$m[i], grid$sigma[i], grid$nu[i], grid$xi[i], w)
  grid[i, "A1"] <- mult[1]
  grid[i, "A2"] <- mult[2]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i] / grid$sd[i]
  grid[i, "frac"] <- (grid$A1[i] * grid$K[i]) / grid$A2[i]
  grid[i, "grp"] <- paste0(as.character(grid$m[i]), as.character(grid$nu[i]), as.character(grid$xi[i]))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = frac, color = as.factor(m))) +
  geom_point() +
  geom_line(aes(group = as.factor(grp), color = as.factor(m))) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "frac", color = "m")
```


#### Conclusion

- `frac` doesn't look dramatically different when idd for mixed distributions by visual inspection.




## Comparison 3: MAV wrt. m

How does MAV depend on $\mu$?  

Note: This question is examined in more detail above under "Diagnosis".

### Signal normal distributed

```{r, eval=FALSE}
p = 3
n = 10000
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

mav <- function(x) {mean(abs(x))}

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_norm_X <- function(n, p, m, sigma) {
  X <- matrix(rnorm(n * p, m, sigma), nrow = n)
  X
}

make_X_hat <- function(X) {
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  set.seed(6235)
  X_hat <- make_X_hat(make_norm_X(n, p, grid$m[i], grid$sigma[i]))
  grid[i, "m"] <- grid$m[i]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = m, y = mav, color = as.factor(sigma))) +
  geom_point() +
  geom_line() +
  #scale_x_log10() +
  #scale_y_log10() +
  labs(x = "mu", y = "mav", color = "sigma") +
  labs(title = "lin-lin")
```



### Signals follow a skewed t-distribution

```{r, eval=FALSE}
p <- 3L
n <- 10000
nu <- 5
xi <- 0.4
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal
nu <- 5
xi <- 0.4

m <- c(-10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

mav <- function(x) {mean(abs(x))}

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_sstd_X <- function(n, p, m, sigma, nu, xi) {
  X <- matrix(rsstd(n * p, mean = m, sd = sigma, nu = nu, xi = xi), nrow = n)
  X
}

make_X_hat <- function(X) {
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  set.seed(6235)
  X_hat <- make_X_hat(make_sstd_X(n, p, grid$m[i], grid$sigma[i], nu, xi))
  grid[i, "m"] <- grid$m[i]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = m, y = mav, color = as.factor(sigma))) +
  geom_point() +
  geom_line() +
  #scale_x_log10() +
  #scale_y_log10() +
  labs(x = "mu", y = "mav", color = "sigma") +
  labs(title = "lin-lin")
```


### Signals follow different t-distributions
The signals are distributed by all permutations of the following parameter values:  

```{r, eval=FALSE}
p <- 3L
n <- 10000
m <- c(-10, -0.1, 0, 0.1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001)
nu <- c(3, 30)
xi <- c(0.2, 0.5, 0.8)
```

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-10, -0.1, 0, 0.1, 10)
sigma <- c(10, 1, 0.1, 0.01, 0.001)
nu <- c(3, 30)
xi <- c(0.2, 0.5, 0.8)
grid <- expand.grid(m = m, sigma = sigma, nu = nu, xi = xi)

seed <- 6354

mav <- function(x) {mean(abs(x))}

## X_hat is a vector of means of all three signals at each time t, i.e. the mean of each row.
## X_hat is the combined signal, here using equal weights.
make_sstd_X <- function(n, p, m, sigma, nu, xi) {
  X <- matrix(rsstd(n * p, mean = m, sd = sigma, nu = nu, xi = xi), nrow = n)
  X
}

make_X_hat <- function(X) {
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  set.seed(seed)
  X_hat <- make_X_hat(make_sstd_X(n, p, grid$m[i], grid$sigma[i], grid$nu[i], grid$xi[i]))
  grid[i, "m"] <- grid$m[i]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "grp"] <- paste0(as.character(grid$sigma[i]), as.character(grid$nu[i]), as.character(grid$xi[i]))
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = m, y = mav, color = as.factor(sigma))) +
  geom_point() +
  geom_line(aes(group = as.factor(grp), color = as.factor(sigma))) +
  #scale_x_log10() +
  #scale_y_log10() +
  labs(x = "mu", y = "mav", color = "sigma") +
  labs(title = "lin-lin")
```

#### Conclusion

- `mav` doesn't look dramatically different when idd for mixed distributions by visual inspection.



## Comparison 4: Contribution of D

Study the contributions of $\mathbf{D}$.  
- $$\frac{\sqrt{\mathbf{w}^T \mathbf{D} \mathbf{H} \mathbf{D} \mathbf{w}}}{\sqrt{\mathbf{w}^T \mathbf{H} \mathbf{w}}}$$
- How bad is it to use $\text{cor}[X]$ instead of $\text{cov}[X]$?

### Signal normal distributed

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

w <- rep(1/p, p)

mav <- function(x) {mean(abs(x))}

make_norm_X <- function(n, p, m, sigma) {
  matrix(rnorm(n * p, m, sigma), nrow = n)
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

make_multipliers <- function(n, p, m, sigma, w) {
  X <- make_norm_X(n, p, m, sigma)
  X_hat <- make_X_hat(X)
  Sigma <- cov(X)
  H <- cor(X)
  D <- sqrt(diag(diag(Sigma)))

  A1 <- sqrt(crossprod(t(w %*% D %*% H %*% D),  w))
  A2 <- sqrt(crossprod(t(w %*% H), w))
  
  list(A1, A2)
}

for(i in 1:nrow(grid)) {
  X_hat <- make_X_hat(make_norm_X(n, p, grid$m[i], grid$sigma[i]))
  mult <- make_multipliers(n, p, grid$m[i], grid$sigma[i], w)
  grid[i, "A1"] <- mult[1]
  grid[i, "A2"] <- mult[2]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "frac"] <- grid$A1[i] / grid$A2[i]
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma, y = frac, color = as.factor(m))) +
  geom_point() +
  geom_line() +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "sigma", y = "frac", color = "m")
```

#### Conclusion

- This shows precisely what we would expect: The contribution of $\mathbf{D}$ is exactly the standard deviation of the individual signal, when all signals are generated from the same distribution.  
- So multiplying the combined signal by the signal multiplication multiplier,  
$$\frac{1}{\sqrt{\mathbf{w}^T \mathbf{H} \mathbf{w}}}$$
is equivalent to multiplying the combined signal by   
$$\frac{\sigma}{\sqrt{\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}}}$$
  - So only when $K = \sigma^{-1}$ does the SDM make strict sense.


## Comparison 5: $K$ vs $\sigma^{-1}$

### Signal normal distributed

```{r, echo=FALSE}
p <- 3L
n <- 10000L ## number of observations, length of signal

m <- c(-100, -10, -1, -0.1, -0.01, - 0.001, 0, 0.001, 0.01, 0.1, 1, 10, 100)
sigma <- c(10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001)
grid <- expand.grid(m = m, sigma = sigma)

w <- rep(1/p, p)

mav <- function(x) {mean(abs(x))}

make_norm_X <- function(n, p, m, sigma) {
  matrix(rnorm(n * p, m, sigma), nrow = n)
}

make_X_hat <- function(X) {
  n <- nrow(X)
  X_hat <- numeric(n)
  for(i in 1:n){X_hat[i] <-mean(X[i, ])}
  X_hat
}

for(i in 1:nrow(grid)) {
  X_hat <- make_X_hat(make_norm_X(n, p, grid$m[i], grid$sigma[i]))
  grid[i, "A1"] <- mult[1]
  grid[i, "A2"] <- mult[2]
  grid[i, "sd"] <- sd(X_hat)
  grid[i, "mav"] <- mav(X_hat)
  grid[i, "K"] <- grid$mav[i] / grid$sd[i]
  grid[i, "sigma_recip"] <- 1/grid$sigma[i]
}
```

```{r, echo=FALSE}
ggplot(grid, aes(x = sigma_recip, y = K, color = as.factor(m))) +
  geom_point() +
  geom_line() +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "1/sigma", y = "K", color = "m")
```


#### Conclusion

- $K$ is generally not equal to $\sigma^{-1}$.
- $K$ is equal to $\sigma^{-1}$ for values of $\mu$ a bit less than $1$, when $\sigma$ is less than 1.
